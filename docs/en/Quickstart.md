# Quickstart

Before running the evaluation script, you need to **configure** the ULMs and set the model_paths properly.

After that, you can use a single script `run.py` to inference and evaluate multiple ULMs and benchmarks at a same time.

## Step 0. Installation & Setup essential keys

**Installation.**

```bash
git clone https://github.com/ULMEvalKit/ULMEvalKit.git
cd ULMEvalKit
pip install -e .
```

See [envs](./docs/envs/README.md) for more requirements of different models and benchmarks.

**Setup Keys.**

To infer with API models (GPT-4v, Gemini-Pro-V, etc.), you need to first setup API keys.
- You can place the required keys in `$ULMEvalKit/.env` or directly set them as the environment variable. If you choose to create a `.env` file, its content will look like:

  ```bash
  # The .env file, place it under $ULMEvalKit
  # Gemini w. Google Cloud Backends
  GOOGLE_API_KEY=
  # OpenAI API
  OPENAI_API_KEY=
  OPENAI_API_BASE=
  # You can also set a proxy for calling api models during the evaluation stage
  EVAL_PROXY=
  ```

- Fill the blanks with your API keys (if necessary). Those API keys will be automatically loaded when doing the inference and evaluation.
## Step 1. Configuration

**ULM Configuration**: All ULMs are configured in `ulmeval/config.py`. During evaluation, you should use the model name specified in `supported_ULM` in `ulmeval/config.py` to select the ULM.

## Step 2. Evaluation

We use `run.py` for inference and evaluation, and `--mode infer` can be used to only perform the inference. Also, `evaluate.py` is provided to only evaluate the results generated by `run.py`.

To use the script, you can use `$ULMEvalKit/run.py`:

**Arguments**

- `--data (list[str])`: Set the dataset names that are supported in ULMEvalKit (names can be found in the codebase README).
- `--model (list[str])`: Set the ULM names that are supported in ULMEvalKit (defined in `supported_ULM` in `ulmeval/config.py`).
- `--mode (str, default to 'all', choices are ['all', 'infer'])`:
  - When `mode` set to `all`, will perform both inference and evaluation;
  - when set to `infer`, will only perform the inference. This is useful when the environment of the ULM and benchmark is not compatible.
- `--api-nproc (int, default to 4)`: The number of threads for OpenAI API calling.
- `--work-dir (str, default to '.')`: The directory to save evaluation results.
- `--num-generations (int, default to None)`: The number of generations per prompt. Default to None, which means the number of generations is determined by the dataset. If the dataset does not specify the number of generations, it defaults to 1.

**Command for Evaluating Benchmarks**

You can run the script with `python` or `torchrun`:

```bash
# When running with `python`, only one ULM instance is instantiated.

# OmniGen2 on T2ICompBench_non_Spatial_VAL, with default number of generations of T2ICompBench (10), Inference and Evalution
python run.py --data T2ICompBench_non_Spatial_VAL --model OmniGen2

# Janus-1.3B on DPGBench, with 5 generations per prompt, Inference only
python run.py --data DPGBench --model Janus-1.3B --num-generations 5 --mode infer

# Janus-1.3B on DPGBench, Evaluation only
python evaluate.py --data DPGBench --model Janus-1.3B --result-file ./outputs/Janus-1.3B/T{date}_G{commit_id}/Janus-1.3B_DPGBench.pkl

# When running with `torchrun`, one ULM instance is instantiated on each GPU. It can speed up the inference.
# However, that is only suitable for ULMs that consume small amounts of GPU memory.

# Janus-Pro-7B on T2ICompBench_non_Spatial_VAL, with 10 generations per prompt. On a node with 8 GPUs.
torchrun --nproc_per_node=8 run.py --data T2ICompBench_non_Spatial_VAL --model Janus-Pro-7B --num-generations 10
```

The evaluation results will be printed as logs, besides. **Result Files** will also be generated in the directory `$YOUR_WORKING_DIRECTORY/{model_name}`. Files ending with `.csv` or `.pkl` contain the evaluated metrics.
